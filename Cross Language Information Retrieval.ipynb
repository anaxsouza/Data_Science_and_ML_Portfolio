{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross Language Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping: File encodings and tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from __future__ import division #To properly handle floating point divisions.\n",
    "import math\n",
    "\n",
    "#Function to tokenise string/sentences.\n",
    "def tokenize(line, tokenizer=word_tokenize):\n",
    "    utf_line = line.decode('utf-8').lower()\n",
    "    return [token.encode('ascii', 'backslashreplace') for token in tokenizer(utf_line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seit',\n",
       " 'damals',\n",
       " 'ist',\n",
       " 'er',\n",
       " 'auf',\n",
       " '\\\\xfcber',\n",
       " '10.000',\n",
       " 'punkte',\n",
       " 'gestiegen',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"Seit damals ist er auf Ã¼ber 10.000 Punkte gestiegen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEVELOPMENT_DOCS = 'data/clir/devel.docs' #Data file for IR engine development\n",
    "\n",
    "DEVELOPMENT_QUERIES = 'data/clir/devel.queries' #Data file containing queries in German\n",
    "\n",
    "DEVELOPMENT_QREL = 'data/clir/devel.qrel' #Data file containing a relevance score or query-doc pairs\n",
    "\n",
    "BITEXT_ENG = 'data/clir/bitext.en' #Bitext data file in English for translation engine and language model development\n",
    "\n",
    "BITEXT_DE = 'data/clir/bitext.de' #Bitext data file in German\n",
    "\n",
    "NEWSTEST_ENG = 'data/clir/newstest.en' #File for testing language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval using [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english')) #converting stopwords to a set for faster processing in the future.\n",
    "stemmer = nltk.stem.PorterStemmer() \n",
    "\n",
    "#Function to extract and tokenize terms from a document\n",
    "def extract_and_tokenize_terms(doc):\n",
    "    terms = []\n",
    "    for token in tokenize(doc):\n",
    "        if token not in stopwords: # 'in' and 'not in' operations are faster over sets than lists\n",
    "            if not re.search(r'\\d',token) and not re.search(r'[^A-Za-z-]',token): #Removing numbers and punctuations \n",
    "                #(excluding hyphenated words)\n",
    "                terms.append(stemmer.stem(token.lower()))\n",
    "    return terms\n",
    "\n",
    "documents = {} #Dictionary to store documents with ids as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading each line in the file and storing it documents dictionary\n",
    "f = open(DEVELOPMENT_DOCS)\n",
    "\n",
    "for line in f:\n",
    "    doc = line.split(\"\\t\")\n",
    "    terms = extract_and_tokenize_terms(doc[1])\n",
    "    documents[doc[0]] = terms\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'name',\n",
       " u'plural',\n",
       " u'ae',\n",
       " u'first',\n",
       " u'letter',\n",
       " u'vowel',\n",
       " u'iso',\n",
       " u'basic',\n",
       " u'latin',\n",
       " u'alphabet',\n",
       " u'similar',\n",
       " u'ancient',\n",
       " u'greek',\n",
       " u'letter',\n",
       " u'alpha',\n",
       " u'deriv',\n",
       " u'upper',\n",
       " u'case',\n",
       " u'version',\n",
       " u'consist']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['290'][:20] #To keep things short, we're only going to check out 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building an inverted index for the documents\n",
    "\n",
    "from collections import defaultdict\n",
    "    \n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "for docid, terms in documents.items():\n",
    "    for term in terms:\n",
    "        inverted_index[term].add(docid)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'121569',\n",
       " '16553',\n",
       " '212541',\n",
       " '228211',\n",
       " '261023',\n",
       " '265975',\n",
       " '276433',\n",
       " '64083',\n",
       " '69930',\n",
       " '72701',\n",
       " '73441',\n",
       " '74323'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index['pizza']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building a TF-IDF representation using BM25 \n",
    "\n",
    "NO_DOCS = len(documents) #Number of documents\n",
    "\n",
    "AVG_LEN_DOC = sum([len(doc) for doc in documents.values()])/len(documents) #Average length of documents\n",
    "\n",
    "#The function below takes the documentid, and the term, to calculate scores for the tf and idf\n",
    "#components, and multiplies them together.\n",
    "def tf_idf_score(k1,b,term,docid):  \n",
    "    \n",
    "    ft = len(inverted_index[term]) \n",
    "    term = stemmer.stem(term.lower())\n",
    "    fdt =  documents[docid].count(term)\n",
    "    \n",
    "    idf_comp = math.log((NO_DOCS - ft + 0.5)/(ft+0.5))\n",
    "    \n",
    "    tf_comp = ((k1 + 1)*fdt)/(k1*((1-b) + b*(len(documents[docid])/AVG_LEN_DOC))+fdt)\n",
    "    \n",
    "    return idf_comp * tf_comp\n",
    "\n",
    "#Function to create tf_idf matrix without the query component\n",
    "def create_tf_idf(k1,b):\n",
    "    tf_idf = defaultdict(dict)\n",
    "    for term in set(inverted_index.keys()):\n",
    "        for docid in inverted_index[term]:\n",
    "            tf_idf[term][docid] = tf_idf_score(k1,b,term,docid)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating tf_idf matrix with said parameter values: k1 and b for all documents.\n",
    "tf_idf = create_tf_idf(1.5,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to retrieve query component\n",
    "def get_qtf_comp(k3,term,fqt):\n",
    "    return ((k3+1)*fqt[term])/(k3 + fqt[term])\n",
    "\n",
    "\n",
    "#Function to retrieve documents || Returns a set of documents and their relevance scores. \n",
    "def retr_docs(query,result_count):\n",
    "    q_terms = [stemmer.stem(term.lower()) for term in query.split() if term not in stopwords] #Removing stopwords from queries\n",
    "    fqt = {}\n",
    "    for term in q_terms:\n",
    "        fqt[term] = fqt.get(term,0) + 1\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for word in fqt.keys():\n",
    "        #print word + ': '+ str(inverted_index[word])\n",
    "        for document in inverted_index[word]:\n",
    "            scores[document] = scores.get(document,0) + (tf_idf[word][document]*get_qtf_comp(0,word,fqt)) #k3 chosen as 0 (default)\n",
    "    \n",
    "    return sorted(scores.items(),key = lambda x : x[1] , reverse=True)[:result_count]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('19961', 12.570721363284687),\n",
       " ('83266', 12.500367334396838),\n",
       " ('266959', 12.46418348068098),\n",
       " ('20206', 12.324327863972716),\n",
       " ('253314', 12.008548114449386)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retr_docs(\"Manchester United\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'manchest',\n",
       " u'unit',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'english',\n",
       " u'profession',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'base',\n",
       " u'old',\n",
       " u'trafford',\n",
       " u'greater',\n",
       " u'manchest',\n",
       " u'play',\n",
       " u'premier',\n",
       " u'leagu',\n",
       " u'found',\n",
       " u'newton',\n",
       " u'heath',\n",
       " u'lyr',\n",
       " u'footbal',\n",
       " u'club',\n",
       " u'club',\n",
       " u'chang',\n",
       " u'name',\n",
       " u'manchest',\n",
       " u'unit',\n",
       " u'move']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['19961'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculating the unigram, bigram and trigram counts. \n",
    "\n",
    "f = open(BITEXT_ENG)\n",
    "\n",
    "train_sentences = []\n",
    "\n",
    "for line in f:\n",
    "    train_sentences.append(tokenize(line))\n",
    "\n",
    "f.close()    \n",
    "\n",
    "#Function to mark the first occurence of words as unknown, for training.\n",
    "def check_for_unk_train(word,unigram_counts):\n",
    "    if word in unigram_counts:\n",
    "        return word\n",
    "    else:\n",
    "        unigram_counts[word] = 0\n",
    "        return \"UNK\"\n",
    "\n",
    "#Function to convert sentences for training the language model.    \n",
    "def convert_sentence_train(sentence,unigram_counts):\n",
    "    #<s1> and <s2> are sentinel tokens added to the start and end, for handling tri/bigrams at the start of a sentence.\n",
    "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_train(token.lower(),unigram_counts) for token in sentence] + [\"</s2>\"]+ [\"</s1>\"]\n",
    "\n",
    "#Function to obtain unigram, bigram and trigram counts.\n",
    "def get_counts(sentences):\n",
    "    trigram_counts = defaultdict(lambda: defaultdict(dict))\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    unigram_counts = {}\n",
    "    for sentence in sentences:\n",
    "        sentence = convert_sentence_train(sentence, unigram_counts)\n",
    "        for i in range(len(sentence) - 2):\n",
    "            trigram_counts[sentence[i]][sentence[i+1]][sentence[i+2]] = trigram_counts[sentence[i]][sentence[i+1]].get(sentence[i+2],0) + 1\n",
    "            bigram_counts[sentence[i]][sentence[i+1]] = bigram_counts[sentence[i]].get(sentence[i+1],0) + 1\n",
    "            unigram_counts[sentence[i]] = unigram_counts.get(sentence[i],0) + 1\n",
    "    unigram_counts[\"</s1>\"] = unigram_counts[\"<s1>\"]\n",
    "    unigram_counts[\"</s2>\"] = unigram_counts[\"<s2>\"]\n",
    "    bigram_counts[\"</s2>\"][\"</s1>\"] = bigram_counts[\"<s1>\"][\"<s2>\"]\n",
    "    return unigram_counts, bigram_counts, trigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_counts, bigram_counts,trigram_counts = get_counts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Constructing unigram model with 'add-k' smoothing\n",
    "token_count = sum(unigram_counts.values())\n",
    "\n",
    "#Function to convert unknown words for testing. \n",
    "#Words that don't appear in the training corpus (even if they are in the test corpus) are marked as UNK.\n",
    "def check_for_unk_test(word,unigram_counts):\n",
    "    if word in unigram_counts and unigram_counts[word] > 0:\n",
    "        return word\n",
    "    else:\n",
    "        return \"UNK\"\n",
    "\n",
    "\n",
    "def convert_sentence_test(sentence,unigram_counts):\n",
    "    return [\"<s1>\"] + [\"<s2>\"] + [check_for_unk_test(word.lower(),unigram_counts) for word in sentence] + [\"</s2>\"]  + [\"</s1>\"]\n",
    "\n",
    "#Returns the log probability of a unigram, with add-k smoothing. We're taking logs to avoid probability underflow.\n",
    "def get_log_prob_addk(word,unigram_counts,k):\n",
    "    return math.log((unigram_counts[word] + k)/ \\\n",
    "                    (token_count + k*len(unigram_counts)))\n",
    "\n",
    "#Returns the log probability of a sentence.\n",
    "def get_sent_log_prob_addk(sentence, unigram_counts,k):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_prob_addk(word, unigram_counts,k) for word in sentence])\n",
    "\n",
    "\n",
    "def calculate_perplexity_uni(sentences,unigram_counts, token_count, k):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += get_sent_log_prob_addk(sentence,unigram_counts,k)\n",
    "    return math.exp(-total_log_prob/test_token_count)\n",
    "\n",
    "\n",
    "f = open(NEWSTEST_ENG)\n",
    "\n",
    "test_sents = []\n",
    "for line in f:\n",
    "    test_sents.append(tokenize(line))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001: 613.918691403\n",
      "0.01: 614.027477551\n",
      "0.1: 615.06903252\n",
      "1: 628.823994251\n",
      "10: 823.302441447\n"
     ]
    }
   ],
   "source": [
    "#Calculating the perplexity for different ks\n",
    "ks = [0.0001,0.01,0.1,1,10]\n",
    "\n",
    "for k in ks:\n",
    "    print str(k) +\": \" + str(calculate_perplexity_uni(test_sents,unigram_counts,token_count,k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculating the N1/N paramaters for Trigrams/Bigrams/Unigrams in Katz-Backoff Smoothing\n",
    "\n",
    "TRI_ONES = 0 #N1 for Trigrams\n",
    "TRI_TOTAL = 0 #N for Trigrams\n",
    "\n",
    "for twod in trigram_counts.values():\n",
    "    for oned in twod.values():\n",
    "        for val in oned.values():\n",
    "            if val==1:\n",
    "                TRI_ONES+=1 #Count of trigram seen once\n",
    "            TRI_TOTAL += 1 #Count of all trigrams seen\n",
    "\n",
    "BI_ONES = 0 #N1 for Bigrams\n",
    "BI_TOTAL = 0 #N for Bigrams\n",
    "\n",
    "for oned in bigram_counts.values():\n",
    "    for val in oned.values():\n",
    "        if val==1:\n",
    "            BI_ONES += 1 #Count of bigram seen once\n",
    "        BI_TOTAL += 1 #Count of all bigrams seen\n",
    "        \n",
    "UNI_ONES = unigram_counts.values().count(1)\n",
    "UNI_TOTAL = len(unigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constructing trigram model with backoff smoothing\n",
    "\n",
    "TRI_ALPHA = TRI_ONES/TRI_TOTAL #Alpha parameter for trigram counts\n",
    "    \n",
    "BI_ALPHA = BI_ONES/BI_TOTAL #Alpha parameter for bigram counts\n",
    "\n",
    "UNI_ALPHA = UNI_ONES/UNI_TOTAL\n",
    "    \n",
    "def get_log_prob_back(sentence,i,unigram_counts,bigram_counts,trigram_counts,token_count):\n",
    "    if trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i],0) > 0:\n",
    "        return math.log((1-TRI_ALPHA)*trigram_counts[sentence[i-2]][sentence[i-1]].get(sentence[i])/bigram_counts[sentence[i-2]][sentence[i-1]])\n",
    "    else:\n",
    "        if bigram_counts[sentence[i-1]].get(sentence[i],0)>0:\n",
    "            return math.log(TRI_ALPHA*((1-BI_ALPHA)*bigram_counts[sentence[i-1]][sentence[i]]/unigram_counts[sentence[i-1]]))\n",
    "        else:\n",
    "            return math.log(TRI_ALPHA*BI_ALPHA*(1-UNI_ALPHA)*((unigram_counts[sentence[i]]+0.0001)/(token_count+(0.0001)*len(unigram_counts)))) \n",
    "        \n",
    "        \n",
    "def get_sent_log_prob_back(sentence, unigram_counts, bigram_counts,trigram_counts, token_count):\n",
    "    sentence = convert_sentence_test(sentence, unigram_counts)\n",
    "    return sum([get_log_prob_back(sentence,i, unigram_counts,bigram_counts,trigram_counts,token_count) for i in range(2,len(sentence))])\n",
    "\n",
    "\n",
    "def calculate_perplexity_tri(sentences,unigram_counts,bigram_counts,trigram_counts, token_count):\n",
    "    total_log_prob = 0\n",
    "    test_token_count = 0\n",
    "    for sentence in sentences:\n",
    "        test_token_count += len(sentence) + 2 # have to consider the end token\n",
    "        total_log_prob += get_sent_log_prob_back(sentence,unigram_counts,bigram_counts,trigram_counts,token_count)\n",
    "    return math.exp(-total_log_prob/test_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461.64686176451505"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the perplexity \n",
    "calculate_perplexity_tri(test_sents,unigram_counts,bigram_counts,trigram_counts,token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating lists of English and German sentences from bitext.\n",
    "\n",
    "from nltk.translate import IBMModel1\n",
    "from nltk.translate import AlignedSent, Alignment\n",
    "\n",
    "eng_sents = []\n",
    "de_sents = []\n",
    "\n",
    "f = open(BITEXT_ENG)\n",
    "for line in f:\n",
    "    terms = tokenize(line)\n",
    "    eng_sents.append(terms)\n",
    "f.close()\n",
    "\n",
    "f = open(BITEXT_DE)\n",
    "for line in f:\n",
    "    terms = tokenize(line)\n",
    "    de_sents.append(terms)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Zipping together the bitexts for easier access\n",
    "paral_sents = zip(eng_sents,de_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building English to German translation table for words (Backward alignment)\n",
    "eng_de_bt = [AlignedSent(E,G) for E,G in paral_sents]\n",
    "eng_de_m = IBMModel1(eng_de_bt, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building German to English translation table for words (Backward alignment)\n",
    "de_eng_bt = [AlignedSent(G,E) for E,G in paral_sents]\n",
    "de_eng_m = IBMModel1(de_eng_bt, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Script below to combine alignments using set intersections\n",
    "combined_align = []\n",
    "\n",
    "for i in range(len(eng_de_bt)):\n",
    "\n",
    "    forward = {x for x in eng_de_bt[i].alignment}\n",
    "    back_reversed = {x[::-1] for x in de_eng_bt[i].alignment}\n",
    "    \n",
    "    combined_align.append(forward.intersection(back_reversed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating German to English dictionary with occurence count of word pairs\n",
    "de_eng_count = defaultdict(dict)\n",
    "\n",
    "for i in range(len(de_eng_bt)):\n",
    "    for item in combined_align[i]:\n",
    "        de_eng_count[de_eng_bt[i].words[item[1]]][de_eng_bt[i].mots[item[0]]] =  de_eng_count[de_eng_bt[i].words[item[1]]].get(de_eng_bt[i].mots[item[0]],0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a English to German dict with occ count of word pais\n",
    "eng_de_count = defaultdict(dict)\n",
    "\n",
    "for i in range(len(eng_de_bt)):\n",
    "    for item in combined_align[i]:\n",
    "        eng_de_count[eng_de_bt[i].words[item[0]]][eng_de_bt[i].mots[item[1]]] =  eng_de_count[eng_de_bt[i].words[item[0]]].get(eng_de_bt[i].mots[item[1]],0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating German to English table with word translation probabilities\n",
    "de_eng_prob = defaultdict(dict)\n",
    "\n",
    "for de in de_eng_count.keys():\n",
    "    for eng in de_eng_count[de].keys():\n",
    "        de_eng_prob[de][eng] = de_eng_count[de][eng]/sum(de_eng_count[de].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating English to German dict with word translation probabilities \n",
    "eng_de_prob = defaultdict(dict)\n",
    "\n",
    "for eng in eng_de_count.keys():\n",
    "    for de in eng_de_count[eng].keys():\n",
    "        eng_de_prob[eng][de] = eng_de_count[eng][de]/sum(eng_de_count[eng].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 0.970873786407767, 'issue': 0.019417475728155338, 'matter': 0.009708737864077669}\n",
      "{'rush': 1.0}\n",
      "{'begins': 0.058823529411764705, 'house': 0.9411764705882353}\n"
     ]
    }
   ],
   "source": [
    "#Examples of translating individual words from German to English\n",
    "print de_eng_prob['frage']\n",
    "\n",
    "print de_eng_prob['handlung']\n",
    "\n",
    "print de_eng_prob['haus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building noisy channel translation model\n",
    "def de_eng_noisy(german):\n",
    "    noisy={}\n",
    "    for eng in de_eng_prob[german].keys():\n",
    "        noisy[eng] = eng_de_prob[eng][german]+ get_log_prob_addk(eng,unigram_counts,0.0001)\n",
    "    return noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'father': -8.798834996562721}\n",
      "{'begins': -10.2208672198799, 'house': -8.163007778647888}\n",
      "{'this': -5.214590799418497, 'the': -3.071527829335362, 'that': -4.664995720177421}\n",
      "{'excuse': -11.870404868087332, 'apology': -12.39683538573032, 'comprehend': -11.89683538573032}\n"
     ]
    }
   ],
   "source": [
    "#Test block to check alignments\n",
    "print de_eng_noisy('vater')\n",
    "print de_eng_noisy('haus')\n",
    "print de_eng_noisy('das')\n",
    "print de_eng_noisy('entschuldigung')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bereue': 1.0}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_de_prob['sorry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "German: der ( von engl . action : tat , handlung , bewegung ) ist ein filmgenre des unterhaltungskinos , in welchem der fortgang der Ã¤uÃeren handlung von zumeist spektakulÃ¤r inszenierten kampf - und gewaltszenen vorangetrieben und illustriert wird .\n",
      "\n",
      "English: the ( , guises . action : indeed , rush , movement ) is a filmgenre the unterhaltungskinos , in much the fortgang the external rush , zumeist spektakul\\xe4r inszenierten fight - and gewaltszenen pushed and illustriert will .\n",
      "\n",
      "\n",
      "116\n",
      "German: die ( einheitenzeichen : u fÃ¼r unified atomic mass unit , veraltet amu fÃ¼r atomic mass unit ) ist eine maÃeinheit der masse .\n",
      "\n",
      "English: the ( einheitenzeichen : u for unified atomic mass unit , obsolete amu for atomic mass unit ) is a befuddled the mass .\n",
      "\n",
      "\n",
      "240\n",
      "German: der von lateinisch actualis , \" wirklich \" , auch aktualitÃ¤tsprinzip , uniformitÃ¤ts - oder gleichfÃ¶rmigkeitsprinzip , englisch uniformitarianism , ist die grundlegende wissenschaftliche methode in der .\n",
      "\n",
      "English: the , lateinisch actualis , `` really `` , , aktualit\\xe4tsprinzip , uniformit\\xe4ts - or gleichf\\xf6rmigkeitsprinzip , english uniformitarianism , is the fundamental scientific method in the .\n",
      "\n",
      "\n",
      "320\n",
      "German: die ( griechisch el , von altgriechisch grc , - \" zusammen - \" , \" anbinden \" , gemeint ist \" die herzbeutel angehÃ¤ngte \" ) , ist ein blutgefÃ¤Ã , welches das blut vom herz wegfÃ¼hrt .\n",
      "\n",
      "English: the ( griechisch el , , altgriechisch grc , - `` together - `` , `` anbinden `` , meant is `` the herzbeutel angeh\\xe4ngte `` ) , is a blutgef\\xe4\\xdf , welches the blood vom heart wegf\\xfchrt .\n",
      "\n",
      "\n",
      "540\n",
      "German: unter der bezeichnung fasst man die drei im nÃ¶rdlichen alpenvorland liegenden gewÃ¤ssereinheiten obersee , untersee und seerhein zusammen .\n",
      "\n",
      "English: under the bezeichnung summarizes one the three , northern alpenvorland liegenden gew\\xe4ssereinheiten obersee , untersee and seerhein together .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Translating first 5 queries into English\n",
    "\n",
    "#Function for direct translation\n",
    "def de_eng_direct(query):\n",
    "    query_english = [] \n",
    "    query_tokens = tokenize(query)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            query_english.append(max(de_eng_prob[token], key=de_eng_prob[token].get))\n",
    "        except:\n",
    "            query_english.append(token) #Returning the token itself when it cannot be found in the translation table.\n",
    "            #query_english.append(\"NA\") \n",
    "    \n",
    "    return \" \".join(query_english)\n",
    "\n",
    "#Function for noisy channel translation\n",
    "def de_eng_noisy_translate(query):  \n",
    "    query_english = [] \n",
    "    query_tokens = tokenize(query)\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        try:\n",
    "            query_english.append(max(de_eng_noisy(token), key=de_eng_noisy(token).get))\n",
    "        except:\n",
    "            query_english.append(token) #Returning the token itself when it cannot be found in the translation table.\n",
    "            #query_english.append(\"NA\") \n",
    "    \n",
    "    return \" \".join(query_english)\n",
    "            \n",
    "f = open(DEVELOPMENT_QUERIES)\n",
    "\n",
    "lno = 0\n",
    "plno = 0\n",
    "\n",
    "#Also building a dictionary of query ids and query content (only for the first 100s)\n",
    "german_qs = {}\n",
    "\n",
    "test_query_trans_sents = [] #Building a list for perplexity checks.\n",
    "\n",
    "for line in f:\n",
    "    lno+=1\n",
    "    query_id = line.split('\\t')[0]\n",
    "    query_german = line.split('\\t')[1]  \n",
    "    \n",
    "    german_qs[query_id] = query_german.strip()\n",
    "    \n",
    "    translation = str(de_eng_noisy_translate(query_german))\n",
    " \n",
    "    if plno<5:\n",
    "        print query_id + \"\\n\" + \"German: \" + str(query_german) + \"\\n\" + \"English: \" + translation +\"\\n\\n\"\n",
    "        plno+=1\n",
    "    test_query_trans_sents.append(translation)\n",
    "    if lno==100:\n",
    "        break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building a dictionary for queryids and relevant document ids\n",
    "qrel = defaultdict(list)\n",
    "\n",
    "f = open(DEVELOPMENT_QREL)\n",
    "\n",
    "for line in f:\n",
    "    item = line.split('\\t')\n",
    "    qrel[item[0]].append(item[2])\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Single function to retreive documents for a German query\n",
    "def trans_retr_docs(german_query,no_of_results,translation_function):\n",
    "    \n",
    "    trans_query = \" \".join(extract_and_tokenize_terms(translation_function(german_query)))\n",
    "    return [item[0] for item in retr_docs(trans_query,no_of_results)] #Retriving 100 documents\n",
    "\n",
    "#Calculating the map score\n",
    "def calc_map(no_of_results,translation_function):\n",
    "    \n",
    "    average_precision = []\n",
    "    \n",
    "    for gq in german_qs.keys():\n",
    "        \n",
    "        relevant_docs = qrel[gq]\n",
    "        incremental_precision = []\n",
    "        resulting_docs = trans_retr_docs(german_qs[gq],no_of_results,translation_function)\n",
    "        \n",
    "        total_counter = 0\n",
    "        true_positive_counter = 0\n",
    "        \n",
    "        for doc in resulting_docs:\n",
    "            total_counter+=1\n",
    "            if doc in relevant_docs:\n",
    "                true_positive_counter += 1\n",
    "                incremental_precision.append(true_positive_counter/total_counter)\n",
    "        \n",
    "        #For no relevant retreivals, the average precision will be considered 0.\n",
    "        try:\n",
    "            average_precision.append(sum(incremental_precision)/len(incremental_precision))\n",
    "        except:\n",
    "            average_precision.append(0)\n",
    "        \n",
    "    return (sum(average_precision)/len(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356571675599\n"
     ]
    }
   ],
   "source": [
    "#Printing the map score for direct translations\n",
    "print calc_map(100,de_eng_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.364795198505\n"
     ]
    }
   ],
   "source": [
    "#Printing the map score for noisy channel translations\n",
    "print calc_map(100,de_eng_noisy_translate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
